"""
Context Window Management System
–û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∂–∏–º–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é
"""
import logging
import tiktoken
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from services.openrouter_service import openrouter_service
from services.ai_memory_service import memory_service

logger = logging.getLogger(__name__)

class ContextWindowManager:
    """
    –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –æ–∫–Ω–æ–º –º–æ–¥–µ–ª–∏
    
    –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
    - –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–∂–∞—Ç–∏–µ –ø—Ä–∏ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–∏ –∫ –ª–∏–º–∏—Ç—É
    - –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–µ—Å—Å–∏–π —Å preserved context
    - –°–≤—è–∑—ã–≤–∞–Ω–∏–µ —Å–µ—Å—Å–∏–π (session chains)
    - –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–π —Ü–µ–ø–æ—á–∫–µ —Å–µ—Å—Å–∏–π
    """
    
    # –õ–∏–º–∏—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    MODEL_LIMITS = {
        "anthropic/claude-3.5-sonnet": 200000,
        "anthropic/claude-3-opus": 200000,
        "anthropic/claude-3-haiku": 200000,
        "openai/gpt-4": 8192,
        "openai/gpt-4-turbo": 128000,
        "openai/gpt-4o": 128000,
        "google/gemini-pro": 32768,
        "google/gemini-2.5-pro": 2000000,
        "default": 8192
    }
    
    # Thresholds –¥–ª—è –¥–µ–π—Å—Ç–≤–∏–π
    COMPRESSION_THRESHOLD = 0.75  # 75% –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è ‚Üí –Ω–∞—á–∞—Ç—å —Å–∂–∞—Ç–∏–µ
    NEW_SESSION_THRESHOLD = 0.90  # 90% ‚Üí —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é
    
    def __init__(self):
        # Initialize tokenizer (using cl100k_base for GPT-4 style counting)
        try:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
        except:
            self.tokenizer = None
            logger.warning("Tiktoken not available, using approximation")
    
    def count_tokens(self, text: str) -> int:
        """–ü–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        if self.tokenizer:
            return len(self.tokenizer.encode(text))
        else:
            # Approximation: ~4 chars per token
            return len(text) // 4
    
    def count_messages_tokens(self, messages: List[Dict]) -> int:
        """–ü–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤–æ –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏—è—Ö"""
        total = 0
        for msg in messages:
            # Count role + content
            total += self.count_tokens(msg.get('role', ''))
            total += self.count_tokens(msg.get('content', ''))
            # Add overhead (role markers, etc)
            total += 4
        return total
    
    async def get_model_limit(self, model: str) -> int:
        """
        –ü–æ–ª—É—á–∏—Ç—å –ª–∏–º–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –º–æ–¥–µ–ª–∏
        –ü—ã—Ç–∞–µ—Ç—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–∑ OpenRouter API, –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç fallback
        """
        try:
            # Try to fetch from OpenRouter API
            models_data = await openrouter_service.get_models()
            
            # Find our model
            for model_data in models_data.get('data', []):
                if model_data.get('id') == model:
                    context_length = model_data.get('context_length', 0)
                    if context_length > 0:
                        logger.info(f"‚úì Model {model} context limit: {context_length} tokens (from OpenRouter)")
                        return context_length
            
            # Model not found in API, use fallback
            logger.warning(f"Model {model} not found in OpenRouter API, using fallback")
            
        except Exception as e:
            logger.warning(f"Failed to fetch model limits from OpenRouter: {str(e)}, using fallback")
        
        # Fallback to hardcoded limits
        return self.MODEL_LIMITS.get(model, self.MODEL_LIMITS["default"])
    
    async def calculate_usage(self, messages: List[Dict], model: str) -> Dict[str, Any]:
        """
        –†–∞—Å—Å—á–∏—Ç–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        
        Returns:
            {
                'current_tokens': int,
                'max_tokens': int,
                'percentage': float,
                'remaining': int,
                'needs_compression': bool,
                'needs_new_session': bool
            }
        """
        current = self.count_messages_tokens(messages)
        maximum = await self.get_model_limit(model)
        percentage = current / maximum if maximum > 0 else 0
        
        return {
            'current_tokens': current,
            'max_tokens': maximum,
            'percentage': percentage,
            'percentage_display': f"{percentage * 100:.1f}%",
            'remaining': maximum - current,
            'needs_compression': percentage >= self.COMPRESSION_THRESHOLD,
            'needs_new_session': percentage >= self.NEW_SESSION_THRESHOLD
        }
    
    async def compress_conversation(
        self,
        messages: List[Dict],
        model: str,
        target_reduction: float = 0.5
    ) -> Tuple[List[Dict], Dict[str, Any]]:
        """
        –£–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
        
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç:
        - System message
        - –ü–æ—Å–ª–µ–¥–Ω–∏–µ N —Å–æ–æ–±—â–µ–Ω–∏–π (—Å–∞–º—ã–µ —Å–≤–µ–∂–∏–µ)
        - –í–∞–∂–Ω—ã–µ —Ñ–∞–∫—Ç—ã –∏ —Ä–µ—à–µ–Ω–∏—è
        
        Args:
            messages: –ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π
            model: –ú–æ–¥–µ–ª—å –¥–ª—è —Å–∂–∞—Ç–∏—è
            target_reduction: –¶–µ–ª–µ–≤–æ–µ —Å–∂–∞—Ç–∏–µ (0.5 = —Å–∂–∞—Ç—å –≤–¥–≤–æ–µ)
        
        Returns:
            (compressed_messages, compression_summary)
        """
        logger.info(f"üóúÔ∏è Starting conversation compression (target: {target_reduction * 100:.0f}% reduction)")
        
        if len(messages) <= 3:
            return messages, {'compressed': False, 'reason': 'Too few messages'}
        
        # Separate system message and conversation
        system_msg = None
        conversation = []
        
        for msg in messages:
            if msg.get('role') == 'system':
                system_msg = msg
            else:
                conversation.append(msg)
        
        # Keep last N messages (most recent context)
        keep_last_n = 4  # Last 2 exchanges
        recent_messages = conversation[-keep_last_n:] if len(conversation) > keep_last_n else conversation
        older_messages = conversation[:-keep_last_n] if len(conversation) > keep_last_n else []
        
        if not older_messages:
            return messages, {'compressed': False, 'reason': 'All messages are recent'}
        
        # Create summary of older messages
        summary = await self._summarize_conversation(older_messages, model)
        
        # Build compressed message list
        compressed = []
        
        if system_msg:
            compressed.append(system_msg)
        
        # Add summary as assistant message
        compressed.append({
            'role': 'assistant',
            'content': f"[Previous conversation summary]\n{summary['summary']}"
        })
        
        # Add recent messages
        compressed.extend(recent_messages)
        
        # Calculate compression stats
        original_tokens = self.count_messages_tokens(messages)
        compressed_tokens = self.count_messages_tokens(compressed)
        reduction = 1 - (compressed_tokens / original_tokens)
        
        compression_info = {
            'compressed': True,
            'original_tokens': original_tokens,
            'compressed_tokens': compressed_tokens,
            'reduction_percentage': reduction * 100,
            'messages_removed': len(older_messages),
            'messages_kept': len(recent_messages),
            'summary': summary['summary']
        }
        
        logger.info(f"‚úì Compressed: {original_tokens} ‚Üí {compressed_tokens} tokens ({reduction * 100:.1f}% reduction)")
        
        return compressed, compression_info
    
    async def _summarize_conversation(
        self,
        messages: List[Dict],
        model: str
    ) -> Dict[str, str]:
        """
        –°–æ–∑–¥–∞—Ç—å —Å–∂–∞—Ç—É—é —Å–≤–æ–¥–∫—É —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤–∞–∂–Ω–æ–≥–æ
        """
        # Format conversation for summarization
        conversation_text = "\n\n".join([
            f"{msg['role'].title()}: {msg['content']}"
            for msg in messages
        ])
        
        summary_prompt = f"""Summarize this conversation, preserving ALL important information:

{conversation_text}

Create a COMPREHENSIVE summary that includes:
1. Key facts mentioned by user (preferences, personal info, decisions)
2. Important questions asked and answers given
3. Tasks/goals discussed
4. Any commitments or promises made
5. Technical details or specific requirements

Be concise but DO NOT lose important context. This summary will replace the conversation.

Format as a flowing paragraph, not bullet points."""
        
        try:
            response = await openrouter_service.chat_completion(
                messages=[{"role": "user", "content": summary_prompt}],
                model=model,
                temperature=0.3,
                max_tokens=1000
            )
            
            summary = response['choices'][0]['message']['content']
            
            return {'summary': summary}
            
        except Exception as e:
            logger.error(f"Summarization error: {str(e)}")
            # Fallback: simple truncation
            return {
                'summary': f"Previous conversation covered: {conversation_text[:500]}..."
            }
    
    async def create_new_session_with_context(
        self,
        current_session_id: str,
        compressed_messages: List[Dict],
        compression_info: Dict
    ) -> Dict[str, Any]:
        """
        –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é —Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        
        Returns:
            {
                'new_session_id': str,
                'parent_session_id': str,
                'context_preserved': str,
                'chain_length': int
            }
        """
        from datetime import datetime
        import uuid
        
        new_session_id = str(uuid.uuid4())
        
        # Store in MongoDB with session chain info
        # (Assuming session_routes handles this)
        
        session_data = {
            'session_id': new_session_id,
            'parent_session_id': current_session_id,
            'created_at': datetime.now().isoformat(),
            'context_summary': compression_info.get('summary', ''),
            'original_session': current_session_id,
            'chain_depth': await self._get_chain_depth(current_session_id) + 1,
            'compressed_messages': compressed_messages
        }
        
        logger.info(f"üîó Created new session {new_session_id} chained from {current_session_id}")
        
        # Remember in AI memory
        await memory_service.remember_conversation(
            user_message=f"Session {current_session_id} reached context limit",
            assistant_response=f"Created new session {new_session_id} with preserved context",
            session_id=new_session_id,
            important=True
        )
        
        return session_data
    
    async def _get_chain_depth(self, session_id: str) -> int:
        """–ü–æ–ª—É—á–∏—Ç—å –≥–ª—É–±–∏–Ω—É —Ü–µ–ø–æ—á–∫–∏ —Å–µ—Å—Å–∏–π"""
        # TODO: Query MongoDB to find parent chain
        # For now, return 0
        return 0
    
    async def get_session_chain(self, session_id: str) -> List[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –≤—Å—é —Ü–µ–ø–æ—á–∫—É —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å–µ—Å—Å–∏–π
        
        Returns:
            [oldest_session_id, ..., current_session_id]
        """
        # TODO: Implement MongoDB query
        # Query parent_session_id recursively
        chain = [session_id]
        
        # Placeholder
        return chain
    
    async def search_across_sessions(
        self,
        session_chain: List[str],
        query: str
    ) -> List[Dict[str, Any]]:
        """
        –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–π —Ü–µ–ø–æ—á–∫–µ —Å–µ—Å—Å–∏–π
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç vector memory –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        –∏–∑ –≤—Å–µ—Ö —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å–µ—Å—Å–∏–π
        """
        results = []
        
        for session_id in session_chain:
            # Search in memory for this session
            memories = await memory_service.recall(
                query=query,
                memory_type="conversation",
                n_results=3
            )
            
            # Filter by session_id
            session_memories = [
                m for m in memories 
                if m.get('metadata', {}).get('session_id') == session_id
            ]
            
            results.extend(session_memories)
        
        # Sort by relevance
        results.sort(key=lambda x: x.get('distance', 999))
        
        return results[:5]  # Top 5 across all sessions
    
    def format_context_warning(self, usage: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–ª—è UI"""
        percentage = usage['percentage']
        
        if percentage >= 0.95:
            return f"‚ö†Ô∏è Context nearly full ({usage['percentage_display']}) - Creating new session..."
        elif percentage >= 0.80:
            return f"‚ö†Ô∏è Context usage high ({usage['percentage_display']}) - Will compress soon"
        elif percentage >= 0.60:
            return f"‚ÑπÔ∏è Context usage: {usage['percentage_display']}"
        else:
            return ""
    
    async def auto_manage_context(
        self,
        messages: List[Dict],
        session_id: str,
        model: str
    ) -> Dict[str, Any]:
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–µ—Ä–µ–¥ –∫–∞–∂–¥—ã–º chat request
        
        Returns:
            {
                'action': 'none' | 'compress' | 'new_session',
                'messages': List[Dict],  # Potentially compressed
                'new_session_id': Optional[str],
                'warning': str
            }
        """
        usage = self.calculate_usage(messages, model)
        
        result = {
            'action': 'none',
            'messages': messages,
            'new_session_id': None,
            'warning': self.format_context_warning(usage),
            'usage': usage
        }
        
        # Check if new session needed
        if usage['needs_new_session']:
            logger.warning(f"‚ö†Ô∏è Context at {usage['percentage_display']} - Creating new session")
            
            # First compress
            compressed_msgs, compression_info = await self.compress_conversation(
                messages, model, target_reduction=0.7
            )
            
            # Create new session
            new_session = await self.create_new_session_with_context(
                session_id,
                compressed_msgs,
                compression_info
            )
            
            result['action'] = 'new_session'
            result['messages'] = compressed_msgs
            result['new_session_id'] = new_session['session_id']
            result['compression_info'] = compression_info
            result['warning'] = f"üîÑ Context full. Started new session with preserved context."
            
        # Check if compression needed (but not new session)
        elif usage['needs_compression']:
            logger.info(f"‚ÑπÔ∏è Context at {usage['percentage_display']} - Compressing")
            
            compressed_msgs, compression_info = await self.compress_conversation(
                messages, model, target_reduction=0.5
            )
            
            result['action'] = 'compress'
            result['messages'] = compressed_msgs
            result['compression_info'] = compression_info
            result['warning'] = f"üóúÔ∏è Compressed conversation to save context space."
        
        return result


# Global instance
context_manager = ContextWindowManager()
